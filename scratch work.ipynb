{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesing on binary classification problems and have data with nominal-valued attributes and no missing\n",
    "values (weather.nominal, titanic, vote.noUnknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArff(filename):\n",
    "    with open ('./NominalData/'+filename+'.arff', 'r') as f:\n",
    "        # split lines, remove ones with comments\n",
    "        lines = [line.lower() for line in f.read().split('\\n') if not line.startswith('%')]\n",
    "        \n",
    "    # remove empty lines\n",
    "    lines = [line for line in lines if line != '']\n",
    "    \n",
    "    columns = []\n",
    "    data = []\n",
    "    for index, line in enumerate(lines):\n",
    "        if line.startswith('@attribute'):\n",
    "            columns.append(line)\n",
    "            \n",
    "        if line.startswith('@data'):\n",
    "            # get the rest of the lines excluding the one that says @data\n",
    "            data = lines[index+1:]\n",
    "            break\n",
    "            \n",
    "    # clean column names -- '@attribute colname  \\t\\t\\t{a, b, ...}'\n",
    "    cleaned_columns = [c[11:c.index('{')].strip() for c in columns]\n",
    "    \n",
    "    # clean and split data\n",
    "    cleaned_data = [d.replace(', ', ',').split(',') for d in data]\n",
    "    \n",
    "    # create dataframe\n",
    "    return pd.DataFrame(cleaned_data, columns = cleaned_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # change class values to {-1, 1}\n",
    "    y, unique = pd.factorize(df.iloc[:,-1])\n",
    "    new_y = np.where(y==0, -1, 1)\n",
    "    assert set(new_y) == {-1, 1}, 'Response variable must be Â±1'\n",
    "    \n",
    "    # change xs to 2d numpy array\n",
    "    xs = df.iloc[:,:-1]\n",
    "    xs = xs.values\n",
    "    \n",
    "    return xs, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_features = np.shape(self.X)[1]\n",
    "        self.info_gain = None\n",
    "        self.error = None\n",
    "        self.best_attribute = None\n",
    "        self.tree = dict()\n",
    "        self.predictions = None\n",
    "        self.class_val = 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"information_gain: {self.info_gain}, error: {self.error}, feature:{self.best_attribute}\"\"\"\n",
    "    \n",
    "    \n",
    "    def _entropy(self, col):\n",
    "        \"\"\"\n",
    "        Calculate the entropy with respect to the target column.\n",
    "        \"\"\"\n",
    "        vals, counts = np.unique(col, return_counts = True)\n",
    "\n",
    "        entropy = np.sum([(-counts[i]/np.sum(counts)) * np.log2(counts[i]/np.sum(counts)) \n",
    "                          for i in range(len(vals))])\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "    def _information_gain(self, attr): \n",
    "        # calculate the entropy of the total dataset\n",
    "        total_entropy = self._entropy(self.y)\n",
    "\n",
    "        # calculate the sum of the weighted entropy of the attributes\n",
    "        vals, counts = np.unique(attr, return_counts=True)\n",
    "\n",
    "\n",
    "        weighted_entropy = sum([(counts[i]/np.sum(counts)) * \n",
    "                            self._entropy(self.y[(attr == vals[i])]) for i in range(len(vals))])\n",
    "\n",
    "        # calculate information gain\n",
    "        info_gain = total_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "    \n",
    "    def _make_tree(self):\n",
    "        # predict values based on self.best_attribute\n",
    "        attr = self.X[:, self.best_attribute]\n",
    "        vals, counts = np.unique(attr, return_counts=True)\n",
    "        \n",
    "        # tree = {attr_val1: p(1), attr_val2, p(1)}\n",
    "        # keys represent branches, values represent probability of 1\n",
    "        # we know the y's are {-1, 1}\n",
    "        for val in vals:\n",
    "            subset = self.y[(attr == val)]\n",
    "            new_subset = np.where(subset == -1, 0, 1) # replace -1 with 0\n",
    "            prob = sum(new_subset) / len(new_subset)\n",
    "            self.tree[val] = prob\n",
    "            \n",
    "    def _predict(self): \n",
    "        # predict values based on self.best_attribute\n",
    "        attr = self.X[:, self.best_attribute]\n",
    "        self.predictions = np.ones(np.shape(self.y))\n",
    "\n",
    "        for i, x_i in enumerate(attr):\n",
    "            if self.tree[x_i] < 0.5:\n",
    "                self.predictions[i] = -1\n",
    "            \n",
    "            if self.tree[x_i] == 0.5:\n",
    "                # pick more probable class from overall class\n",
    "                self.predictions[i] = np.sign(np.sum(self.y))\n",
    "    \n",
    "    \n",
    "    def _calculate_error(self):        \n",
    "        self._make_tree()\n",
    "        self._predict()\n",
    "        \n",
    "        # calculate percent inaccuracy\n",
    "#         assert np.shape(self.predictions) == np.shape(self.y) # sanity check\n",
    "#         accuracy = np.sum(self.predictions == self.y, axis=0) / len(self.y)\n",
    "#         self.error = 1 - accuracy\n",
    "    \n",
    "    def learn(self):\n",
    "        max_gain = float('-inf')\n",
    "\n",
    "        for f in range(self.n_features):\n",
    "            gain = self._information_gain(self.X[:, f])\n",
    "            \n",
    "            if max_gain < gain:\n",
    "                self.info_gain = gain\n",
    "                self.best_attribute = f\n",
    "                max_gain = gain\n",
    "        self._calculate_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, T=20):\n",
    "        self.T = T\n",
    "    \n",
    "    def _resample(self, X, y, w):\n",
    "        # sanity checks \n",
    "        assert sum(w) > 0.999999\n",
    "        assert sum(w) < 1.000001\n",
    "\n",
    "        assert np.shape(w)[0] == X.shape[0]\n",
    "\n",
    "        # combine into dataframe\n",
    "        xs = pd.DataFrame(X)\n",
    "        df = pd.concat([xs, pd.DataFrame(y, columns=[\"y\"])], axis=1)\n",
    "\n",
    "        # resample\n",
    "        new_data = df.sample(frac=1, replace=True, weights=w, random_state=1)\n",
    "        new_xs = new_data.iloc[:,:-1].values\n",
    "        new_ys = new_data.iloc[:, -1].values\n",
    "        return new_xs, new_ys\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        epsilon = 1e-6 # add stability -- avoid div_by_0 error for when learner.error = 0\n",
    "\n",
    "        n_instances = np.shape(X)[0]\n",
    "        self.stumps = np.zeros(shape=self.T, dtype=object)\n",
    "        self.alphas = np.zeros(shape=self.T)\n",
    "\n",
    "        # initialize weights uniformly\n",
    "        weights = np.ones(shape=n_instances) / n_instances\n",
    "        # automatically use the whole data for first sample\n",
    "        # since all weights are even\n",
    "#         sample_X = X\n",
    "#         sample_y = y\n",
    "\n",
    "        for t in range(self.T):\n",
    "            # use updated weights to resample\n",
    "            sample_X, sample_y = self._resample(X, y, weights)\n",
    "            \n",
    "            learner = DecisionStump(sample_X, sample_y)\n",
    "            learner.learn()\n",
    "#             print(f\"Iteration: {t}, Missed: {np.sum(np.where(sample_y != learner.predictions, 1, 0))}\")\n",
    "\n",
    "            error = sum(weights[sample_y != learner.predictions]) / sum(weights)\n",
    "            if error > 0.5:\n",
    "                error = 1 - error\n",
    "                learner.class_val = -1\n",
    "        \n",
    "            alpha = 0.5 * np.log((1.0 - error + epsilon) / (error + epsilon))\n",
    "\n",
    "            weights *= np.exp(-alpha * sample_y * learner.predictions * learner.class_val)\n",
    "            weights /= weights.sum()\n",
    "            \n",
    "#             sample_X, sample_y = self._resample(X, y, weights)\n",
    "            \n",
    "            # save stump objects and alphas\n",
    "            self.stumps[t] = learner\n",
    "            self.alphas[t] = alpha\n",
    "            \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "\n",
    "        # For each classifier => label the samples\n",
    "        for stump, alpha in zip(self.stumps, self.alphas):\n",
    "            attr = X[:, stump.best_attribute]\n",
    "            predictions = np.ones(np.shape(y_pred)) # Set all predictions to '1' initially\n",
    "            \n",
    "            for i, x_i in enumerate(attr):\n",
    "                val = stump.tree.get(x_i, None) # guard against lookup not in tree for attribute\n",
    "\n",
    "                if val is None or val < 0.5:\n",
    "#                 if stump.tree[x_i] < 0.5:\n",
    "                    predictions[i] = -1 # switch predictions to -1 if p(1) < 0.5\n",
    "\n",
    "            # Add predictions weighted by the classifiers alpha (alpha indicative of classifier's proficiency)\n",
    "            y_pred += alpha * predictions * stump.class_val\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('weather.nominal'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7794943820224719"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('titanic'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698275862068966"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('vote.noUnknowns'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_val(filename, T, k):\n",
    "    X, y = preprocess_data(readArff(filename))\n",
    "\n",
    "    # Group examples s.t. fold sizes differ by at most 1.\n",
    "    # Assign [N = floor(len(data) / k)] examples to each fold.\n",
    "    # Assign one additional example to [r = len(data) % k] folds.\n",
    "    groups_X = []\n",
    "    groups_y = []\n",
    "    size_per_group = len(X) // k\n",
    "    r = len(X) % k\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        n_examples = size_per_group + (i < r)\n",
    "        groups_X.append(X[start: start + n_examples])\n",
    "        groups_y.append(y[start: start + n_examples])\n",
    "        start += n_examples\n",
    "\n",
    "    # cross validation each fold\n",
    "    total_correct = total_data = 0\n",
    "    for i in range(k):\n",
    "        print(\"\\nUsing group {} of {} as test data\".format(i+1, k))\n",
    "        train_data_X = np.array([x for group in groups_X[:i] + groups_X[i+1:] for x in group])\n",
    "        train_data_y = np.array([x for group in groups_y[:i] + groups_y[i+1:] for x in group])\n",
    "        test_data_X = np.array(groups_X[i])\n",
    "        test_data_y = np.array(groups_y[i])\n",
    "\n",
    "        ada = AdaBoost(T)\n",
    "        ada.train(train_data_X, train_data_y)\n",
    "        preds = ada.predict(test_data_X)\n",
    "        n_correct = np.sum(preds == test_data_y, axis=0)\n",
    "        n_data = len(preds)\n",
    "        total_correct += n_correct\n",
    "        total_data += n_data\n",
    "\n",
    "    avg_acc = 100 * total_correct / total_data\n",
    "    print(\"\\nAverage accuracy: {:.2f}% ({}/{})\"\n",
    "          .format(avg_acc, total_correct, total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using group 1 of 14 as test data\n",
      "\n",
      "Using group 2 of 14 as test data\n",
      "\n",
      "Using group 3 of 14 as test data\n",
      "\n",
      "Using group 4 of 14 as test data\n",
      "\n",
      "Using group 5 of 14 as test data\n",
      "\n",
      "Using group 6 of 14 as test data\n",
      "\n",
      "Using group 7 of 14 as test data\n",
      "\n",
      "Using group 8 of 14 as test data\n",
      "\n",
      "Using group 9 of 14 as test data\n",
      "\n",
      "Using group 10 of 14 as test data\n",
      "\n",
      "Using group 11 of 14 as test data\n",
      "\n",
      "Using group 12 of 14 as test data\n",
      "\n",
      "Using group 13 of 14 as test data\n",
      "\n",
      "Using group 14 of 14 as test data\n",
      "\n",
      "Average accuracy: 64.29% (9/14)\n"
     ]
    }
   ],
   "source": [
    "k_fold_cross_val('weather.nominal', 25, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using group 1 of 10 as test data\n",
      "\n",
      "Using group 2 of 10 as test data\n",
      "\n",
      "Using group 3 of 10 as test data\n",
      "\n",
      "Using group 4 of 10 as test data\n",
      "\n",
      "Using group 5 of 10 as test data\n",
      "\n",
      "Using group 6 of 10 as test data\n",
      "\n",
      "Using group 7 of 10 as test data\n",
      "\n",
      "Using group 8 of 10 as test data\n",
      "\n",
      "Using group 9 of 10 as test data\n",
      "\n",
      "Using group 10 of 10 as test data\n",
      "\n",
      "Average accuracy: 77.95% (555/712)\n"
     ]
    }
   ],
   "source": [
    "k_fold_cross_val('titanic', 25, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using group 1 of 10 as test data\n",
      "\n",
      "Using group 2 of 10 as test data\n",
      "\n",
      "Using group 3 of 10 as test data\n",
      "\n",
      "Using group 4 of 10 as test data\n",
      "\n",
      "Using group 5 of 10 as test data\n",
      "\n",
      "Using group 6 of 10 as test data\n",
      "\n",
      "Using group 7 of 10 as test data\n",
      "\n",
      "Using group 8 of 10 as test data\n",
      "\n",
      "Using group 9 of 10 as test data\n",
      "\n",
      "Using group 10 of 10 as test data\n",
      "\n",
      "Average accuracy: 96.98% (225/232)\n"
     ]
    }
   ],
   "source": [
    "k_fold_cross_val('vote.noUnknowns', 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
