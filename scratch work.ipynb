{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tesing on binary classification problems and have data with nominal-valued attributes and no missing\n",
    "values (weather.nominal, titanic, vote.noUnknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArff(filename):\n",
    "    with open ('./NominalData/'+filename+'.arff', 'r') as f:\n",
    "        # split lines, remove ones with comments\n",
    "        lines = [line.lower() for line in f.read().split('\\n') if not line.startswith('%')]\n",
    "        \n",
    "    # remove empty lines\n",
    "    lines = [line for line in lines if line != '']\n",
    "    \n",
    "    columns = []\n",
    "    data = []\n",
    "    for index, line in enumerate(lines):\n",
    "        if line.startswith('@attribute'):\n",
    "            columns.append(line)\n",
    "            \n",
    "        if line.startswith('@data'):\n",
    "            # get the rest of the lines excluding the one that says @data\n",
    "            data = lines[index+1:]\n",
    "            break\n",
    "            \n",
    "    # clean column names -- '@attribute colname  \\t\\t\\t{a, b, ...}'\n",
    "    cleaned_columns = [c[11:c.index('{')].strip() for c in columns]\n",
    "    \n",
    "    # clean and split data\n",
    "    cleaned_data = [d.replace(', ', ',').split(',') for d in data]\n",
    "    \n",
    "    # create dataframe\n",
    "    return pd.DataFrame(cleaned_data, columns = cleaned_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # change class values to {-1, 1}\n",
    "    y, unique = pd.factorize(df.iloc[:,-1])\n",
    "    new_y = np.where(y==0, -1, 1)\n",
    "    assert set(new_y) == {-1, 1}, 'Response variable must be Â±1'\n",
    "    \n",
    "    # change xs to 2d numpy array\n",
    "    xs = df.iloc[:,:-1]\n",
    "    xs = xs.values\n",
    "    \n",
    "    return xs, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_features = np.shape(self.X)[1]\n",
    "        self.info_gain = None\n",
    "        self.error = None\n",
    "        self.best_attribute = None\n",
    "        self.tree = dict()\n",
    "        self.predictions = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"information_gain: {self.info_gain}, error: {self.error}, feature:{self.best_attribute}\"\"\"\n",
    "    \n",
    "    \n",
    "    def _entropy(self, col):\n",
    "        \"\"\"\n",
    "        Calculate the entropy with respect to the target column.\n",
    "        \"\"\"\n",
    "        vals, counts = np.unique(col, return_counts = True)\n",
    "\n",
    "        entropy = np.sum([(-counts[i]/np.sum(counts)) * np.log2(counts[i]/np.sum(counts)) \n",
    "                          for i in range(len(vals))])\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "    def _information_gain(self, attr): \n",
    "        # calculate the entropy of the total dataset\n",
    "        total_entropy = self._entropy(self.y)\n",
    "\n",
    "        # calculate the sum of the weighted entropy of the attributes\n",
    "        vals, counts = np.unique(attr, return_counts=True)\n",
    "\n",
    "\n",
    "        weighted_entropy = sum([(counts[i]/np.sum(counts)) * \n",
    "                            self._entropy(self.y[(attr == vals[i])]) for i in range(len(vals))])\n",
    "\n",
    "        # calculate information gain\n",
    "        info_gain = total_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "    \n",
    "    def _make_tree(self):\n",
    "        # predict values based on self.best_attribute\n",
    "        attr = self.X[:, self.best_attribute]\n",
    "        vals, counts = np.unique(attr, return_counts=True)\n",
    "        \n",
    "        # tree = {attr_val1: p(1), attr_val2, p(1)}\n",
    "        # keys represent branches, values represent probability of 1\n",
    "        # we know the y's are {-1, 1}\n",
    "        for val in vals:\n",
    "            subset = self.y[(attr == val)]\n",
    "            new_subset = np.where(subset == -1, 0, 1) # replace -1 with 0\n",
    "            prob = sum(new_subset) / len(new_subset)\n",
    "            self.tree[val] = prob\n",
    "            \n",
    "    def _predict(self): \n",
    "        # predict values based on self.best_attribute\n",
    "        attr = self.X[:, self.best_attribute]\n",
    "        self.predictions = np.ones(np.shape(self.y))\n",
    "\n",
    "        for i, x_i in enumerate(attr):\n",
    "            if self.tree[x_i] < 0.5:\n",
    "                self.predictions[i] = -1\n",
    "            # if == 0.5 then could break tie with majority over everything -- add in at the end\n",
    "    \n",
    "    \n",
    "    def _calculate_error(self):        \n",
    "        self._make_tree()\n",
    "        self._predict()\n",
    "        \n",
    "        # calculate percent inaccuracy\n",
    "        assert np.shape(self.predictions) == np.shape(self.y) # sanity check\n",
    "        accuracy = np.sum(self.predictions == self.y, axis=0) / len(self.y)\n",
    "        self.error = 1 - accuracy\n",
    "    \n",
    "    def learn(self):\n",
    "        max_gain = float('-inf')\n",
    "\n",
    "        for f in range(self.n_features):\n",
    "            gain = self._information_gain(self.X[:, f])\n",
    "            \n",
    "            if max_gain < gain:\n",
    "                self.info_gain = gain\n",
    "                self.best_attribute = f\n",
    "                max_gain = gain\n",
    "        self._calculate_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, T=200):\n",
    "        self.T = T\n",
    "    \n",
    "    def _resample(self, X, y, w):\n",
    "        # sanity checks \n",
    "        assert sum(w) > 0.999999\n",
    "        assert sum(w) < 1.000001\n",
    "\n",
    "        assert np.shape(w)[0] == X.shape[0]\n",
    "\n",
    "        # combine into dataframe\n",
    "        xs = pd.DataFrame(X)\n",
    "        df = pd.concat([xs, pd.DataFrame(y, columns=[\"y\"])], axis=1)\n",
    "\n",
    "        # resample\n",
    "        new_data = df.sample(frac=1, replace=True, weights=w, random_state=1)\n",
    "        new_xs = new_data.iloc[:,:-1].values\n",
    "        new_ys = new_data.iloc[:, -1].values\n",
    "        return new_xs, new_ys\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        epsilon = 1e-6 # add stability -- avoid div_by_0 error for when learner.error = 0\n",
    "\n",
    "        n_instances = np.shape(X)[0]\n",
    "        self.stumps = np.zeros(shape=self.T, dtype=object)\n",
    "        self.alphas = np.zeros(shape=self.T)\n",
    "\n",
    "        # initialize weights uniformly\n",
    "        weights = np.ones(shape=n_instances) / n_instances\n",
    "        # automatically use the whole data for first sample\n",
    "        # since all weights are even\n",
    "        sample_X = X\n",
    "        sample_y = y\n",
    "\n",
    "        for t in range(self.T):\n",
    "            learner = DecisionStump(sample_X, sample_y)\n",
    "            learner.learn()\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - learner.error + epsilon) / (learner.error + epsilon))\n",
    "\n",
    "            weights *= np.exp(-alpha * sample_y * learner.predictions)\n",
    "            weights /= weights.sum()\n",
    "\n",
    "            # use updated weights to resample\n",
    "            sample_X, sample_y = self._resample(X, y, weights)\n",
    "            \n",
    "            # save stump objects and alphas\n",
    "            self.stumps[t] = learner\n",
    "            self.alphas[t] = alpha\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "\n",
    "        # For each classifier => label the samples\n",
    "        for stump, alpha in zip(self.stumps, self.alphas):\n",
    "            attr = X[:, stump.best_attribute]\n",
    "            predictions = np.ones(np.shape(y_pred)) # Set all predictions to '1' initially\n",
    "            \n",
    "            for i, x_i in enumerate(attr):\n",
    "                val = stump.tree.get(x_i, None) # guard against lookup not in tree for attribute\n",
    "                if val is None or val < 0.5:\n",
    "                    predictions[i] = -1 # switch predictions to -1 if p(1) < 0.5\n",
    "\n",
    "            # Add predictions weighted by the classifiers alpha (alpha indicative of classifier's proficiency)\n",
    "            y_pred += alpha * predictions\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('weather.nominal'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6264044943820225"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('titanic'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698275862068966"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = preprocess_data(readArff('vote.noUnknowns'))\n",
    "ada = AdaBoost()\n",
    "ada.train(X,y)\n",
    "preds = ada.predict(X)\n",
    "accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
